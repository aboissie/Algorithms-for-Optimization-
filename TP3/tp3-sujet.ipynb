{"cells":[{"cell_type":"markdown","metadata":{"deletable":false},"source":["***\n","**Algorithmes d'optimisation -- L3 MINT et doubles licences 2019/2020 -- Université Paris-Saclay**\n","***\n","$\\newcommand{\\Rsp}{\\mathbb{R}} \\newcommand{\\nr}[1]{\\|#1\\|} \\newcommand{\\abs}[1]{|#1|} \\newcommand{\\eps}{\\varepsilon} \\newcommand{\\sca}[2]{\\langle#1|#2\\rangle} \\newcommand{\\D}{\\mathrm{D}} \\newcommand{\\hdots}{\\dots} \\newcommand{\\cond}{\\mathrm{cond}}$\n","On commence par importer les modules habituels:"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","# la commande suivante agrandit les figures\n","plt.rcParams['figure.figsize'] = [9.,6.]\n","\n","def verifier_gradient(f,g,x0):\n","    N = len(x0)\n","    gg = np.zeros(N)\n","    for i in range(N):\n","        eps = 1e-4\n","        e = np.zeros(N)\n","        e[i] = eps\n","        gg[i] = (f(x0+e) - f(x0-e))/(2*eps)\n","    print('erreur numerique dans le calcul du gradient: %g (doit etre petit)' % np.linalg.norm(g(x0)-gg))"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["# TP 3: Gradient projeté et pénalisation \n","\n","## Partie I: Problème d'obstacle par gradient projeté\n","\n","On considère un système physique constitué d'une chaine de $N+1$ ressorts. Les deux extrémités du $i$ème ressort ($0\\leq i\\leq N$ sont les points $(t_i,x_i) \\in\\Rsp^2$ et $(t_{i+1},x_{i+1}) \\in \\Rsp^2$, où $t_i = hi$ est fixé.  On considère également que la chaine est fixée à ses extrémités: $x_0 = x_{N+1} = 0$. Il reste donc $N$ inconnues $x = (x_1,\\hdots,x_N)\\in \\Rsp^N$. L'énergie du système est donnée par la formule suivante:\n","\n","$$J(x) = J(x_1,\\hdots,x_N) = \\frac{1}{2}\\sum_{0\\leq i\\leq N} \\nr{x_{i+1} - x_i}^2 $$ \n","\n","où l'on a donc fixé $x_0 = x_{N+1} = 0$. On pose un obstacle sous la chaîne de ressort, qui force chacune des coordonnées $x_i$ a être minorée par une constante $f_i$ (on peut penser à une main qui pousse le ressort par exemple). On arrive dont au problème d'optimisation\n","\n","$$ \\min_{x\\in K} J(x) \\hbox{ où } K = \\{x\\in \\Rsp^N \\mid \\forall 1\\leq i\\leq N, x_i \\geq f_i \\}. $$\n","\n","Nous allons la résolution numérique de ce problème d'optimisation par la méthode de gradient projeté. On rappelle les formules suivantes:\n","\n","- La projection d'un point $y\\in\\Rsp^N$ sur le convexe fermé $K$ est donnée par \n","\n","$$ P_K(y) = (\\max(y_1,f_1),\\hdots,\\max(y_N,f_N)) $$\n","\n","- On admet que la fonction $J$ peut être mise sous la forme\n","\n","$$ J(x) = \\frac{1}{2} \\sca{x}{Q x}  \\hbox{ où } Q = \\begin{pmatrix}2 & -1  & 0 &\\cdots & 0 \\\\\n","-1 & 2 & -1 & \\ddots & \\vdots   \\\\ \n","0 & \\ddots & \\ddots & \\ddots& \\vdots \\\\\n","\\vdots & \\ddots & -1 & 2 & -1 \\\\\n","0 & \\hdots & 0 & -1 & 2\n","\\end{pmatrix}$$\n","\n","Le gradient de la fonction  $J$ en $x\\in \\Rsp^N$ est alors donné par $\\nabla J(x) = Q x.$\n","\n","**Q1)** Écrire une fonction projK(x) retournant la projection de $x\\in \\Rsp^N$ sur $K$ (la tester avec des petites valeurs de $N$). On fixe désormais $N=30$: écrire la matrice $Q$ et deux fonctions `J(x)` et `gradJ(x)` calculant la valeur et le gradient de $J$. Vérifier le calcul du gradient avec `check_gradient`."]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# initialiser les variables K,Q, x0, fmin\n","\n","N = 30\n","T = np.linspace(0,1,N+2)[1:-1]\n","F = np.exp(-50*(T-.75)**2) + np.exp(-50*(T-.25)**2)\n","#plt.plot(T,F,'.-g',label='obstacle')\n","plt.fill_between(T,F,'.-g',label='obstacle',alpha=0.1)\n","plt.legend()\n","\n","# <completer>\n"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["On rappelle que l'algorithme du gradient projeté est défini de la manière suivante:\n","\n","$$ \\begin{cases}\n","x^{(0)} \\in \\Rsp^N\\\\\n","x^{(k+1)} = P_K\\left(x^{(k)} - \\tau \\nabla J(x^{(k)})\\right)\n","\\end{cases} $$\n","\n","où $\\tau>0$. On a démontré en cours que si \n","\n","$$ \\forall x\\in \\Rsp^M, m\\mathrm{Id} \\leq \\D^2 J(x) \\leq M\\mathrm{Id} , $$\n","\n","alors l'algorithme converge dès que $\\tau < 2m/M^2$, avec un taux optimal lorsque $\\tau^* = m/M^2$.\n","\n","**Q2)** Montrer que $\\D^2 J(x) = Q$; calculer les valeurs propres de $Q$ via la fonction np.linalg.eigvalsh. Quelle valeur de $\\tau^*$ l'estimation ci-dessus donne-t-elle? "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# <completer>\n"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["**Q3)** Écrire une boucle réalisant l'algorithme du gradient projeté (pour $1\\leq k\\leq 500$), et stockant le vecteur $G=(\\nr{x^{(k+1)} - x^{(k)}})$ (comme il s'agit d'un algorithme de point fixe, c'est une bonne manière de quantifier la convergence). Tracer la solution $x \\in \\Rsp^N$ trouvée toutes les 20 itérations (on suggère plt.plot(T,x)).  Tester pour $\\tau = \\tau^*$. En pratique, vérifier que l'algorithme converge toujours $\\tau=0.5$ mais diverge pour $\\tau$ trop grand."]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# <completer>\n"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["**Q4**) Comme $\\nabla J(x) = Qx$, l'algorithme peut en fait être décrit par\n","\n","$$ \\begin{cases}\n","x^{(0)} \\in \\Rsp^N\\\\\n","x^{(k+1)} = P_K(A_\\tau x^{(k)}) \n","\\end{cases} $$\n","\n","où $A_\\tau = \\mathrm{Id}_N - \\tau Q$. Montrer que si toutes les valeurs propres de $A_\\tau$ sont de module $<1$, alors l'application $x\\mapsto A_\\tau x$ est contractante. Vérifier numériquement ce critère pour $\\tau=0.5$ (on utilisera np.linalg.eigvalsh pour calculer les valeurs propres)."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# <completer>\n"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["# Problème d'obstacle par pénalisation\n","\n","Une autre manière d'approcher la solution d'un problème d'optimisation sous contrainte consiste à *pénaliser* la violation des contraintes. Plus précisément, on rajoute à la fonctionnelle $J$ un terme par contrainte d'inégalité $x_i >= F_i$, de la forme $P_i(x) = \\frac{1}{\\eps} \\max(F_i - x_i, 0)^2$ où $\\eps>0$: \n","\n","- Si le point $x = (x_1,\\hdots,x_N)$ vérifie $x_i\\geq F_i$, alors $P_i(x) = 0$. \n","- Par contre, si $x_i < F_i$ (et ne satisfait pas la contrainte), alors $P_i(x) \\geq \\frac{1}{\\eps} (F_i - x_i)^2$\n","\n","Ainsi, on considère le problème de minimisation\n","\n","$$P_\\eps := \\min_{x\\in\\Rsp^N} J_\\eps(x) \\hbox{ où } J_\\eps(x) = J(x) + \\frac{1}{\\eps} \\sum_{1\\leq i\\leq N} \\max(F_i - x_i,0)^2. $$\n","\n","on admettra que \n","\n","$$\\nabla J_\\eps(x) = \\nabla J(x) - \\frac{2}{\\eps} \\sum_{1\\leq i\\leq N} \\max(F_i - x_i, 0) e_i $$\n","\n","\n","**Q1)** Écrire deux fonctions `Jeps`/`gradJeps` calculant $J_\\eps/\\nabla J_\\eps$. Vérifier le calcul du gradient en utilisant `verifier_gradient`."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def Jeps(x,eps):\n","    # <completer>\n","    \n","def gradJeps(x,eps):\n","    # <completer>\n","\n","x0 = np.random.rand(N)\n","verifier_gradient(lambda x: Jeps(x,1), \n","                  lambda x: gradJeps(x,1),\n","                  x0)\n"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["**Q2)** Résoudre le problème $P_\\eps$ par descente de gradient avec rebroussement (on fournit la fonction `gradient_armijo`) pour $\\eps=1,0.1,0.01,0.001$. Interpréter l'explosion du nombre d'itérations."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def rebroussement_armijo(f,x,d,m,alpha=0.3,beta=0.5):\n","    t = 1\n","    while f(x+t*d) > f(x) + alpha*t*m:\n","        t = beta*t\n","    return t\n","\n","def gradient_armijo(f,g,x0,err=1e-5,maxiter=2000):\n","    x = x0.copy()\n","    fiter = []\n","    giter = []\n","    k = 0 # nombre d'itérations\n","    while(True): \n","        k = k+1\n","        if k > maxiter: # maximum de 10^6 itérations\n","            print('erreur: nombre maximum d\\'itérations atteint')\n","            break\n","        d = -g(x)\n","        fiter.append(f(x))\n","        giter.append(np.linalg.norm(d))\n","        if np.linalg.norm(d) <= err:\n","            break\n","        t = rebroussement_armijo(f,x,d,-np.linalg.norm(d)**2)\n","        #if k%10==0: # on affiche des informations toute les 20 itérations\n","        #    print('iteration %d: f=%f, |g|=%f, step=%f' % (k, f(x), np.linalg.norm(d),t))\n","        x = x + t*d\n","    return x,np.array(fiter),np.array(giter)\n","\n","# <completer>\n"]}],"metadata":{"celltoolbar":"None","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":1}
