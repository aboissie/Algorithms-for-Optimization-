{"cells":[{"cell_type":"markdown","metadata":{"deletable":false},"source":["***\n","**Algorithmes d'optimisation -- L3 MINT et doubles licences 2019/2020 -- Université Paris-Saclay**\n","***\n","\n","# TP 3 (bis): Projection sur le simplexe et optimisation de portefeuille\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.optimize\n","%matplotlib inline\n","# la commande suivante agrandit les figures\n","plt.rcParams['figure.figsize'] = [9.,6.]\n","\n","def verifier_gradient(f,g,x0):\n","    N = len(x0)\n","    gg = np.zeros(N)\n","    for i in range(N):\n","        eps = 1e-4\n","        e = np.zeros(N)\n","        e[i] = eps\n","        gg[i] = (f(x0+e) - f(x0-e))/(2*eps)\n","    print('erreur numerique dans le calcul du gradient: %g (doit etre petit)' % np.linalg.norm(g(x0)-gg))"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["## I. Projection sur le simplexe\n","$\\newcommand{\\Rsp}{\\mathbb{R}}$\n","$\\newcommand{\\sca}[2]{\\langle #1|#2\\rangle}$\n","$\\newcommand{\\eps}{\\varepsilon}$\n","$\\newcommand{\\proj}{\\mathrm{proj}}$\n","Comme dans le troisième exercice de la feuille de TD consacrée au calcul de projections, on appelle simplexe l'ensemble \n","\n","$$\\Delta = \\{ x\\in \\Rsp_+^n \\mid \\sum_{1\\leq i \\leq n} x_i = 1\\}.$$\n","\n","Dans un premier temps, on va chercher à calculer la projection d'un point \n","$y\\in\\Rsp^n$ sur $\\Delta$. Pour cela, on admettra les deux résultats suivants, démontrés dans le TD:\n","\n","- il existe $\\kappa\\in\\Rsp$ tel que $\\sum_{1\\leq i\\leq n} \\max(y_i - \\kappa, 0) = 1$\n","- la projection de $y$ sur $\\Delta$ s'écrit alors $\\proj_\\Delta(y) = (\\max(y_i - \\kappa, 0))_{1\\leq i\\leq n}$\n","\n","Ainsi, pour trouver la projection d'un point $y\\in\\Rsp^n$ sur le simplexe $\\Delta$, il suffit de trouver $\\kappa\\in\\Rsp$ tel que $g(\\kappa) = 0$ où l'on a posé\n","$$ g(\\kappa) = \\left(\\sum_{1\\leq i\\leq n} \\max(y_i - \\kappa, 0)\\right) -1$$\n","\n","\n","**Q1)** Soit $y = (0.1,1.5,2.1) \\in \\Rsp^3$. Écrire la fonction `g(kappa)` décrite ci-dessus. Trouver $\\kappa$ vérifiant $g(\\kappa) = 1$ en utilisant la fonction `scipy.optimize.root(g,x0=0,method='anderson').x`."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# <completer>\n"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["**Q2)** En s'inspirant du code de la fonction précédente, écrire une fonction proj_simplexe calculant la projection d'un point $y\\in\\Rsp^n$ sur $\\Delta$. Pour vérifier le bon fonctionnement de cette fonction, calculer calculer $p=$`proj_simplexe(y)` pour $y=$`np.random.randn(n)` puis vérifier que \n","\n","$$ \\forall i\\in\\{1,\\dots,n\\}, \\sca{y - p}{p - e_i} \\geq 0,$$\n","\n","où $e_i$ est le $i$ème vecteur de la base canonique.\n","\n","*(Question subsidiaire: pourquoi cette inégalité doit-elle être vraie pour $p = \\proj_\\Delta(y)$ ? Caractérise-t-elle la projection sur $\\Delta$?)*\n","<!-- (dans le cas $n=2$, on peut également tirer quelques points aléatoirement dans le plan et visualiser le segment qui les relie à leur projection sur $\\Delta$) -->"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def proj_simplexe(y):\n","    # <completer>\n","\n","# validation: calcul de produits scalaires\n","# <completer>\n"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["## II. Optimisation de portefeuille.\n","\n","Dans cette partie, il s'agit de résoudre un problème d'optimisation sous contraintes de la forme suivante:\n","\n","$$ \\min_{x\\in\\Delta} f(x) \\hbox{ où } f(x) = \\frac{1}{2}\\sca{x}{Qx} + \\frac{1}{2\\eta}(\\sca{r}{x}-r_0)^2, $$\n","\n","où $Q \\in \\mathcal{M}_{n,n}(\\Rsp)$ est une matrice symétrique définie positive, $r\\in\\Rsp$ est un vecteur et $r_0\\in\\Rsp$ sont donnés. Dans la suite, on fixera $\\eta = 10^{-2}$. \n","\n","**Motivation:** Ce problème modélise une situation où un investisseur cherche placer un portefeuille en garantissant un certain niveau de rendement $r_0\\in\\Rsp$ tout en minimisant le risque. Plus précisément, dans ce problème $n$ est le nombre d'actifs (actions, etc.), et la variable $x\\in\\Rsp^n$ décrit la stratégie d'investissement: $x_i$ décrit la fraction du portefeuille que l'on investit dans l'actif $i$. Ainsi, la contrainte $x\\in\\Delta$ modélise:\n","\n","- qu'on investit une fraction $x_i$ positive dans chaque actif (contrainte $x_i\\geq 0$);\n","- et que la somme totale disponible est investie ($\\sum_i x_i = 1$).\n","\n","La matrice $Q$ modélise la covariance entre les actifs:\n","- si $Q_{ij}\\geq 0$, cela signifie que les actifs financiers $i$ et $j$ sont positivement correlés, et il est donc risqué d'investir dans les deux en même temps. \n","- au contraire, si $Q_{i,j}<0$, les actifs sont négativement correlés, et en investissant dans les deux on diminue le risque. \n","Le problème d'optimisation qu'on regarde consiste donc à trouver une stratégie d'investissement ($x\\in \\Delta$) cherchant à viser un rendement donné (terme $\\frac{1}{2\\eta}(\\sca{r}{x}-r_0)^2$) tout en minimisant le risque (terme $\\sca{x}{Qx}$).\n","\n","En pratique, on considèrera les données suivantes:"]},{"cell_type":"code","execution_count":5,"metadata":{"scrolled":true},"outputs":[],"source":["if False: # mettre True pour changer les dates, les actifs, etc\n","    import yfinance as yf \n","    import numpy as np\n","    stocks = ['NFLX','MSFT','BA','AIR']\n","    closes = np.array([np.array(yf.download(s,'2019-01-01','2020-01-01').Close) for s in stocks]).T\n","    r = (closes[-1,:] - closes[1,:])/closes[1,:] # rendement sur l'année 2019\n","    Q = np.cov(closes - np.mean(closes,0),rowvar=False) # estimation de la covariance entre les valeurs\n","\n","Q = np.array([[1199.6242199,  -225.74269344,  270.42617708, -112.31853678],\n","              [-225.74269344,  224.42514399, -157.75776414,   46.31290714],\n","              [ 270.42617708, -157.75776414,  600.37115079,  -28.1665365 ],\n","              [-112.31853678,   46.31290714,  -28.1665365,    21.77422792]])\n","r = np.array([0.20888442, 0.55953316, 0.00602209, 0.2042723])\n","r0 = 0.7*np.max(r)\n","eta = 1e-4"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["**Q1)** Montrer que la fonction $f$ est strictement convexe et que\n","$\\nabla f(x) = Qx + \\frac{1}{\\eta} (\\sca{r}{x}-r_0) r.$"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["## II.1. Méthode de gradient projeté\n","\n","**Q2)** Écrire une fonction `f` et `gradf`, et utiliser la fonction `verifier_gradient` pour valider l'implémentation."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# <completer>\n","\n","# vérification du calcul du gradient\n","verifier_gradient(f,gradf,np.random.rand(len(r)))"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["**Q3)** Implémenter l'algorithme du gradient projeté pour résoudre le problème avec les données ci-dessus, i.e.\n","\n","$$ \\begin{cases} x^{(0)} = 0_{\\Rsp^n}\\\\\n","x^{(k+1)} = \\mathrm{proj\\_simplexe}(x - \\tau \\nabla f(x^{(k)})) \n","\\end{cases}$$ \n","\n","Tracer sur deux figures distinctes:\n","- La suite des valeurs $f(x^{(k)})$ pour $1\\leq k<100$.\n","- La suite $\\|{x^{k} - x^{(k-1)}}\\|$ pour $1\\leq k<100$ (on pourra mettre les abscisses en échelle logarithmique). Pourquoi la convergence de cette suite vers zéro est-elle une indication du bon fonctionnement de l'algorithme?\n","\n","Pour choisir le paramètre $\\tau$ de l'algorithme du gradient projeté, on pourra procéder par tâtonnement: par exemple, choisir $\\tau$ de sorte à ce que $k\\mapsto f(x^{(k)})$ soit décroissante et que la suite $\\|{x^{k} - x^{(k-1)}}\\|$ semble tendre vers zéro."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# <completer>\n"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["## II.2. Méthode de pénalisation\n","\n","**Q1** Montrer que le simplexe $\\Delta = \\{ x\\in\\Rsp_+^n\\mid \\sum_i x_i = 1\\}$ peut-être mis sous la forme\n","\n","$$ \\Delta = \\{ x\\in\\Rsp^n \\mid \\forall 1\\leq i\\leq \\ell, c_i(x) \\leq 0 \\} $$\n","\n","avec $\\ell = n+2$ et \n","\n","$$ c_i(x) = \\begin{cases} -x_i & \\hbox{ si } 1\\leq i\\leq n \\\\\n","x_1+\\dots+x_n - 1 & \\hbox{ si } i=n+1\\\\\n","-(x_1+\\dots+x_n - 1) & \\hbox{ si } i=n+2\\end{cases} $$\n","\n","Dans la méthode de pénalisation, le problème d'optimisation sous contraintes \n","\n","$$ P =  \\min_{x\\in\\Delta} f(x) \\hbox{ où } f(x) = \\frac{1}{2}\\sca{x}{Qx} + \\frac{1}{2\\eta}(\\sca{r}{x}-r_0)^2, $$\n","\n","est alors approché par le problème d'optimisation sans contraintes suivant:\n","\n","$$ P_\\eps = \\min_{x\\in\\Rsp^n} f_\\eps(x) \\hbox{ où } f_\\eps(x) = f(x) + \\frac{1}{\\eps}\\sum_{1\\leq i\\leq \\ell} \\max(c_i(x),0)^2 $$\n","\n","**Q2** Montrer que \n","\n","$$ f_\\eps(x) = f(x) + \\frac{1}{\\eps}\\left(\\sum_{1\\leq i\\leq n} \\max(-x_i,0)^2)\\right) + \\frac{1}{\\eps}(x_1+\\dots+x_n - 1)^2 $$\n","$$(\\nabla f_\\eps(x))_i = (\\nabla f(x))_i - \\frac{2}{\\eps} \\max(-x_i,0) + \\frac{2}{\\eps} (x_1+\\dots+x_n - 1)$$\n","\n","Coder deux fonctions `feps(x)` et `gradfeps(x)`, et vérifier leur bon fonctionnement en utilisant `verifier_gradient`."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["eps = 1e-3\n","\n","# <completer>\n","\n","verifier_gradient(feps,gradfeps,10*(np.random.rand(len(r))-.5))"]},{"cell_type":"markdown","metadata":{"deletable":false},"source":["**Q3** En utilisant la fonction `gradient_armijo` ci-dessous, vérifier le fonctionnement de cette approche pour des valeurs de `eps` modérées ($\\eps = 10^{-2}$ où $10^{-3}$). Commenter les résultats (respect des contraintes, vitesse de convergence)."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def gradient_armijo(f,gradf,x0,err=1e-5,maxiter=20000):\n","    x = x0.copy()\n","    fiter = []\n","    giter = []\n","    k = 0 # nombre d'itérations\n","    while(True): \n","        k = k+1\n","        if k > maxiter: # maximum de 10^6 itérations\n","            print('erreur: nombre maximum d\\'itérations atteint')\n","            break\n","        d = -gradf(x)\n","        fiter.append(f(x))\n","        giter.append(np.linalg.norm(d))\n","        if np.linalg.norm(d) <= err:\n","            break\n","        t = 1\n","        m = -np.dot(d,d)\n","        while f(x+t*d) > f(x) + 0.3*t*m:\n","            t = 0.5*t\n","        if k%100==0: # on affiche des informations toute les 100 itérations\n","            print('iteration %d: f=%f, |g|=%f, step=%f' % (k, f(x), np.linalg.norm(d),t))\n","        x = x + t*d\n","    return x,np.array(fiter),np.array(giter)\n","\n","# <completer>\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["array([0.12850422, 0.48099323, 0.04045281, 0.36423465])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["1.0141849153713878"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["np.sum(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"None","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":2}
